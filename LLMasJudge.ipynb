{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from apikey import api_key\n",
    "from myTools import *\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a text literal called greetings with value \"Hello\" and display greetings on the dashboard as a label.   ```envision\n",
      "greeting = \"Hello\" // define the text literal\n",
      "show label greeting // show the text literal as a label. There should be no 'with' !\n",
      "``` +++\n",
      "title = \"Envision Language\"\n",
      "url = \"language\"\n",
      "description = \"Envision is the Domain-Specific Language (DSL) engineered by Lokad for the specific purpose of the predictive optimization of supply chains. This document is not intended for complete programming beginners, but rather for an audience already familiar with basic programming patterns like Microsoft Excel formulas.\"\n",
      "weight = 1\n",
      "alwaysopen = false\n",
      "+++\n",
      "\n",
      "Envision is the Domain-Specific Language (DSL) engineered by Lokad for the specific purpose of the predictive optimization of supply chains. This document is not intended for complete programming beginners, but rather for an audience already familiar with basic programming patterns like Microsoft Excel formulas.\n",
      "\n",
      "Envision has been designed since day 1 with one key feature in mind: the possibility to perform automated script rewrites if the syntax were to evolve. During the first 5 years of operations, Lokad performed around one hundred incremental rewrites. Those rewrites ensure that all our client companies benefit from the latest version of Envision without having to manually revise their scripts. There are many areas in Envisionâ€™s syntax that we plan to adjust in the future. In this documentation, the areas of evolution that have already been clearly identified are pointed out in the _Roadmap_ notes.\n",
      "\n",
      "Unlike many scripting languages, Envision focuses on delivering a high-degree of _correctness by design_, which means capturing as many issues as possible at **compile time** (the moment when the script is _compiled_) rather than **runtime** (the moment when the script is _run_). Capturing issues at compile time is preferable because whenever the amount of processed data is sizable, a runtime issue can take a long time (several minutes) to manifest itself causing productivity and production reliability problems. This documentation focuses on the _compile-time_ angles of Envision.\n",
      "\n",
      "**Table of contents**\n",
      "{{< toc >}}{{< /toc >}}\n"
     ]
    }
   ],
   "source": [
    "docu =read_file(\"docs/envision-brief.md\")    \n",
    "challenge = read_file(\"mychallenges/\"+\"c000\"+\".md\")\n",
    "\n",
    "# decompose challenge into question and prof_answer\n",
    "\n",
    "question,prof_answer,references=decompose_challenge(challenge)\n",
    "ref_str=create_ref(references)\n",
    "print(question,prof_answer,ref_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```envision\n",
      "greetings = \"Hello\"\n",
      "show label greetings\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "coder_personality=\"You are a proficient coder in the Domain Specific Language called Envision. \\\n",
    "    Your task is to generate response to the given challenge. \\\n",
    "    Some challenges will ask you to generate Envision code,\\\n",
    "    others will ask you to explain given code or answer questions related to the Envision language. \\\n",
    "    Do not output any intermediate thinking or explanation, only give the final answer.\\\n",
    "    Here is the documentation of Envision:\\\n",
    "    ### Documentation\\n\" + docu\n",
    "coder_prompt=question\n",
    "\n",
    "coder_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": coder_personality},\n",
    "        {\"role\": \"user\", \"content\": coder_prompt}\n",
    "    ],\n",
    "    max_tokens=1000,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.4,\n",
    ")\n",
    "stud_sentence=coder_response.choices[0].message.content\n",
    "print(stud_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### QUESTION: define a text literal called greetings with value \"Hello\" and display greetings on the dashboard as a label.  \n",
      "### PROFESSOR ANSWER: ```envision\n",
      "greeting = \"Hello\" // define the text literal\n",
      "show label greeting // show the text literal as a label. There should be no 'with' !\n",
      "```\n",
      "### STUDENT ANSWER: ```envision\n",
      "greetings = \"Hello\"\n",
      "show label greetings\n",
      "```\n",
      "The student's answer is ACCEPTABLE.\n",
      "\n",
      "- The student correctly defines a text literal called `greetings` with the value \"Hello\".\n",
      "- The student correctly displays the `greetings` text literal on the dashboard as a label.\n",
      "\n",
      "Therefore, the student's answer fulfills the requirements of the question and is logically correct.\n",
      "\n",
      "The student's answer is ACCEPTABLE.\n",
      "\n",
      "### Judgement: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this personality sticks more to professor's answer.\n",
    "\n",
    "judge_personality_teacherAuthority=\"Your goal is to judge the correctness of STUDENT ANSWER, as an answer to the QUESTION.\\\n",
    "In order to judge the STUDENT ANSWER, you are given the PROFESSOR ANSWER with a piece of related documentation.\\\n",
    "Your main job is not to check the syntax correctness, but the logical correctness.\\\n",
    "If the STUDENT ANSWER does not treat the QUESTION logically, it is UNACCEPTABLE.\\\n",
    "Pay special attention to the comments in the PROFESSOR ANSWER. If these comments include\\\n",
    "a rule and if the STUDENT ANSWER violates it, this is UNACCEPTABLE.\\\n",
    "If in the show command, the STUDENT ANSWER add or omit a print position (like a1b2) compared to the PROFESSOR ANSWER, ignore this: this is always ACCEPTABLE.\\\n",
    "The use of extra variable or table to temporarily contain a intermediate quantity is ACCEPTABLE.\\\n",
    "Differences in variable names, column names, table names and label names etc. shall systematically be ACCEPTABLE! \\\n",
    "There are sometimes various ways or logics to treat the same QUESTION, and this is ACCEPTABLE, as long as the goal of the QUESTION is achieved.\\\n",
    "Let's think aloud step by step before making your judgement. Tell each ACCEPTABLE or UNACCEPTABLE point. \\\n",
    "At the end of your output, you should judge 0 if there is anything UNACCEPTABLE (even only 1 mark of UNACCEPTABLE) in the STUDENT ANSWER;\\\n",
    "and judge 1 if everything is ACCEPTABLE. End your judgment by the digit either 0 or 1. \\\n",
    "Here is the piece of related documentation : \\n ## DOCUMENTATION\\n\" + ref_str\n",
    "\n",
    "judge_prompt = \"### QUESTION: \"+question+\"\\n### PROFESSOR ANSWER: \"+prof_answer+\"\\n### STUDENT ANSWER: \"+stud_sentence\n",
    "\n",
    "# Generate a response from the chatbot\n",
    "judge_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": judge_personality_teacherAuthority},\n",
    "        {\"role\": \"user\", \"content\": judge_prompt}\n",
    "    ],\n",
    "    max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.1,\n",
    ")\n",
    "print(judge_prompt)\n",
    "# Print the generated response\n",
    "print(judge_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "verifier_personality=\"Your task is to summarize the input given by the judge:\\\n",
    "    - If the judge has found nothing unacceptable, you should output 1.\\\n",
    "    - If the judge has found anything unacceptable, you should output 0.\\\n",
    "    - Focus on the last line of the judge's sentence: if it has already announced the final judgement, you should output the same (0 or 1).\\\n",
    "    Do not output anything other than pur digit 0 or 1, without font, punctuation or any special character.\"\n",
    "verifier_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": verifier_personality},\n",
    "        {\"role\": \"user\", \"content\": judge_response.choices[0].message.content}\n",
    "    ],\n",
    "    max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.05,\n",
    ")\n",
    "print(verifier_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greetings = \"Hello\"\n",
      "show label greetings\n"
     ]
    }
   ],
   "source": [
    "# extract the 'real' code from the student answer (cut away the '''envision bit at the start and end)\n",
    "def extract_code(stud_sentence):\n",
    "    lines = stud_sentence.strip().split('\\n')\n",
    "    return '\\n'.join(lines[1:-1])\n",
    "print(extract_code(stud_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "# send code to online compiler and check if it compiles\n",
    "\n",
    "def check_compilation(script):\n",
    "    url = \"https://try.lokad.com/w/script/trycompile\"\n",
    "    payload = {\n",
    "        \"Script\": script\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send POST request\n",
    "        response = requests.post(url, json=payload)\n",
    "\n",
    "        # Check for successful response\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if result[\"IsCompOk\"]:\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Compilation Failed!\")\n",
    "                for message in result[\"CompMessages\"]:\n",
    "                    print(f\"Error: {message['Text']} (Line: {message['Line']}, Start: {message['Start']}, Length: {message['Length']}, Severity: {message['Severity']})\")\n",
    "                    return False\n",
    "        else:\n",
    "            print(\"Error: Unable to reach the compilation service.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "check_compilation(extract_code(stud_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each question, try 3 generation-compilations.\n",
    "# if compiles, further check with judge.\n",
    "def pipeline_verify(challenge,coder_personality,judge_personality=judge_personality_teacherAuthority):\n",
    "\n",
    "    question,prof_answer,references=decompose_challenge(challenge)\n",
    "    ref_str=create_ref(references)    \n",
    "    n_tries=3\n",
    "\n",
    "    for compile_try in range(n_tries):\n",
    "        coder_prompt=question\n",
    "        coder_response = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": coder_personality},\n",
    "                {\"role\": \"user\", \"content\": coder_prompt}\n",
    "            ],\n",
    "            max_tokens=1000,  # Adjust the number of tokens based on your needs\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        stud_sentence=coder_response.choices[0].message.content\n",
    "        if (question.split(\"\\n\")[0] ==\\\n",
    "        '# this question expects a textual answer and not generation of code. #'):\n",
    "            print('# theoretical question, no compile.')\n",
    "            break\n",
    "        if(check_compilation(extract_code(stud_sentence))):\n",
    "            print('# compile ok')\n",
    "            break\n",
    "        elif (compile_try==n_tries-1):\n",
    "            print( \"# too many failures !\")\n",
    "            print('# badcode:\\n'+extract_code(stud_sentence))\n",
    "            return stud_sentence,\"too many failures !\",False\n",
    "\n",
    "    judge_prompt = \"### QUESTION: \"+question+\"\\n### PROFESSOR ANSWER: \"+prof_answer+\"\\n### STUDENT ANSWER: \"+stud_sentence\n",
    "    judge_response = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": judge_personality+ref_str},\n",
    "            {\"role\": \"user\", \"content\": judge_prompt}\n",
    "        ],\n",
    "        max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    judge_sentence=judge_response.choices[0].message.content\n",
    "    verifier_response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": verifier_personality},\n",
    "        {\"role\": \"user\", \"content\": judge_sentence}\n",
    "    ],\n",
    "    max_tokens=800,  # Adjust the number of tokens based on your needs\n",
    "    temperature=0.05,\n",
    ")\n",
    "    judge_decision=verifier_response.choices[0].message.content=='1'\n",
    "    print ('# judge_decision:',judge_decision)\n",
    "    if not judge_decision:\n",
    "        print('# badcode:\\n',extract_code(stud_sentence))\n",
    "        print('# judge explanation:\\n',judge_sentence)\n",
    "    return stud_sentence,judge_sentence,judge_decision\n",
    "# a all-in-one function to score a model on a list of challenges\n",
    "def pipeline_score_allchallenge(indexes,coder_personality):\n",
    "    challenges=[read_file(\"mychallenges/\"+index+\".md\") for index in indexes]\n",
    "    score=0\n",
    "    for i in range(len(challenges)):\n",
    "        challenge=challenges[i]\n",
    "        print('\\n### verifying challenge No. '+indexes[i])\n",
    "        _,_,judge_decision=pipeline_verify(challenge,coder_personality)\n",
    "        if (judge_decision): score+=1\n",
    "    print('correct:'+str(score)+' out of '+str(len(challenges))+', '+str(score/len(challenges)*100)+'%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### verifying challenge No. c000\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c001\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c002\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c003\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c004\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c005\n",
      "# theoretical question, no compile.\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c006\n",
      "# compile ok\n",
      "# judge_decision: False\n",
      "# badcode:\n",
      " table T = with\n",
      "  [| as Name, as Score |]\n",
      "  [| \"Alice\", 85 |]\n",
      "  [| \"Bob\", 92 |]\n",
      "  [| \"Charlie\", 88 |]\n",
      "  [| \"David\", 90 |]\n",
      "  [| \"Eve\", 87 |]\n",
      "\n",
      "maxScore = max(T.Score)\n",
      "bestName = argmax(T.Name, T.Score)\n",
      "\n",
      "show scalar \"Max Score\" a1b2 with maxScore\n",
      "show scalar \"Best Name\" c1d2 with bestName\n",
      "# judge explanation:\n",
      " The student answer is **UNACCEPTABLE**.\n",
      "\n",
      "1. The student defined the table columns as \"Name\" and \"Score\", while the professor defined them as \"name\" and \"score\". This difference in column names is **ACCEPTABLE**.\n",
      "   \n",
      "2. The student used the `argmax` function incorrectly. The `argmax` function in Envision takes the value to compare as the first argument and the index that we want to know as the second argument. In this case, the student should compare the scores to find the best name, but the student compared the names instead. This logic error makes the student answer **UNACCEPTABLE**.\n",
      "\n",
      "3. The student used the `show scalar` command to display the results, while the professor used the `show label` command. This difference in the display command is **ACCEPTABLE**.\n",
      "\n",
      "Therefore, the final judgment is **0**.\n",
      "\n",
      "### verifying challenge No. c007\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Function 'argmax' does not take 1 arguments. (Line: 15, Start: 3, Length: 6, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "table Students = with\n",
      "  [| as Name, as Teacher, as Score |]\n",
      "  [| \"Alice\", \"John Doe\", 85 |]\n",
      "  [| \"Bob\", \"Jane Smith\", 70 |]\n",
      "  [| \"Charlie\", \"John Doe\", 92 |]\n",
      "  [| \"Diana\", \"John Doe\", 78 |]\n",
      "  [| \"Eve\", \"Jane Smith\", 88 |]\n",
      "\n",
      "Students.Successful = Students.Score > 79\n",
      "\n",
      "show table \"Successful Students of John Doe\" a1c5 with\n",
      "  Students[Teacher == \"John Doe\" and Successful]\n",
      "  mean(Students.Score[Teacher == \"John Doe\" and Successful]) as \"Mean Score\"\n",
      "  max(Students.Score[Teacher == \"John Doe\" and Successful]) as \"Best Score\"\n",
      "  argmax(Students.Score[Teacher == \"John Doe\" and Successful]) as \"Best Student\"\n",
      "\n",
      "### verifying challenge No. c008\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c009\n",
      "Compilation Failed!\n",
      "Error: Found '.' but expected end-of-line, 'into', operator5, operator4, operator3, operator2, operator1, operator0, 'default' or 'where'. (Line: 13, Start: 46, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found '.' but expected end-of-line, 'into', operator5, operator4, operator3, operator2, operator1, operator0, 'default' or 'where'. (Line: 13, Start: 46, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found '.' but expected end-of-line, 'into', operator5, operator4, operator3, operator2, operator1, operator0, 'default' or 'where'. (Line: 13, Start: 46, Length: 1, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "table Catalog[item] = with\n",
      "  [| as item, as itemcolor |]\n",
      "  [| \"shirt\", \"blue\" |]\n",
      "  [| \"hat\", \"red\" |]\n",
      "  [| \"shoes\", \"green\" |]\n",
      "\n",
      "table ColorPrices[color] = with\n",
      "  [| as color, as price |]\n",
      "  [| \"blue\", 20 |]\n",
      "  [| \"red\", 15 |]\n",
      "  [| \"green\", 25 |]\n",
      "\n",
      "Catalog.itemprice = ColorPrices[Catalog.item].price\n",
      "\n",
      "show table \"Catalog with Prices\" a1b4 with\n",
      "  Catalog.item\n",
      "  Catalog.itemcolor\n",
      "  Catalog.itemprice\n",
      "\n",
      "### verifying challenge No. c010\n",
      "Compilation Failed!\n",
      "Error: Found 'where' but expected boolean, number, dedent, 'order', 'group', 'enum', 'schema', identifier, markdown template start, text interpolation, '(', 'if', unaryoperator or text. (Line: 19, Start: 3, Length: 5, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found 'where' but expected boolean, number, dedent, 'order', 'group', 'enum', 'schema', identifier, markdown template start, text interpolation, '(', 'if', unaryoperator or text. (Line: 19, Start: 3, Length: 5, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found 'where' but expected boolean, number, dedent, 'order', 'group', 'enum', 'schema', identifier, markdown template start, text interpolation, '(', 'if', unaryoperator or text. (Line: 19, Start: 3, Length: 5, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "table Catalog = with\n",
      "  [| as Item, as OrderDate, as DeliveryDate |]\n",
      "  [| \"item1\", date(2022, 1, 1), date(2022, 1, 10) |]\n",
      "  [| \"item2\", date(2022, 2, 5), date(2022, 2, 15) |]\n",
      "  [| \"item3\", date(2022, 3, 10), date(2022, 3, 25) |]\n",
      "  [| \"item4\", date(2022, 4, 15), date(2022, 4, 30) |]\n",
      "  [| \"item5\", date(2022, 5, 20), date(2022, 6, 5) |]\n",
      "  [| \"item6\", date(2022, 6, 25), date(2022, 7, 10) |]\n",
      "  [| \"item7\", date(2022, 7, 30), date(2022, 8, 15) |]\n",
      "  [| \"item8\", date(2022, 8, 5), date(2022, 8, 25) |]\n",
      "  [| \"item9\", date(2022, 9, 10), date(2022, 9, 30) |]\n",
      "  [| \"item10\", date(2022, 10, 15), date(2022, 11, 5) |]\n",
      "\n",
      "Catalog.Leadtime = Catalog.DeliveryDate - Catalog.OrderDate\n",
      "\n",
      "show table \"Items with Leadtime > 20 days\" a1c11 with\n",
      "  Catalog.Item\n",
      "  Catalog.Leadtime\n",
      "  where Catalog.Leadtime > 20\n",
      "\n",
      "### verifying challenge No. c011\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. c012\n",
      "# compile ok\n",
      "# judge_decision: True\n",
      "\n",
      "### verifying challenge No. mc011\n",
      "Compilation Failed!\n",
      "Error: Found 'cross' but expected 'show', 'with', 'for', 'each', '_', identifier, 'write', 'schema', 'montecarlo', 'autodiff', '@', 'def', 'delete', 'expect', '[|', 'read', '{', 'table', 'loop', 'import', 'return', 'dash', 'sample', 'const', 'export', 'if', 'span', 'where', 'keep' or end-of-script. (Line: 9, Start: 1, Length: 5, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found 'cross' but expected 'show', 'with', 'for', 'each', '_', identifier, 'write', 'schema', 'montecarlo', 'autodiff', '@', 'def', 'delete', 'expect', '[|', 'read', '{', 'table', 'loop', 'import', 'return', 'dash', 'sample', 'const', 'export', 'if', 'span', 'where', 'keep' or end-of-script. (Line: 9, Start: 1, Length: 5, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found 'cross' but expected 'show', 'with', 'for', 'each', '_', identifier, 'write', 'schema', 'montecarlo', 'autodiff', '@', 'def', 'delete', 'expect', '[|', 'read', '{', 'table', 'loop', 'import', 'return', 'dash', 'sample', 'const', 'export', 'if', 'span', 'where', 'keep' or end-of-script. (Line: 9, Start: 1, Length: 5, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "keep span date = [date(2005, 1, 1) .. date(2005, 1, 31)]\n",
      "\n",
      "table Products = with\n",
      "  [| as Name, as Origin, as Factor |]\n",
      "  [| \"ProductA\", \"USA\", 1.2 |]\n",
      "  [| \"ProductB\", \"France\", 0.8 |]\n",
      "  [| \"ProductC\", \"Germany\", 1.0 |]\n",
      "\n",
      "cross Products, date with\n",
      "  Quantity = random.poisson(10 into Products)\n",
      "\n",
      "### verifying challenge No. mc012\n",
      "Compilation Failed!\n",
      "Error: Found '.' but expected 'group', 'as', '{', 'write', identifier, end-of-line, 'into', operator5, operator4, operator3, operator2, operator1, operator0, ',', 'default' or 'order'. (Line: 7, Start: 58, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found '.' but expected 'group', 'as', '{', 'write', identifier, end-of-line, 'into', operator5, operator4, operator3, operator2, operator1, operator0, ',', 'default' or 'order'. (Line: 7, Start: 58, Length: 1, Severity: Error)\n",
      "Compilation Failed!\n",
      "Error: Found '.' but expected 'group', 'as', '{', 'write', identifier, end-of-line, 'into', operator5, operator4, operator3, operator2, operator1, operator0, ',', 'default' or 'order'. (Line: 7, Start: 58, Length: 1, Severity: Error)\n",
      "# too many failures !\n",
      "# badcode:\n",
      "table Orders[Pid] = with\n",
      "  [| as Pid, as Date, as Quantity |]\n",
      "  [| \"apple\",  date(2020, 4, 15), 3 |]\n",
      "  [| \"pear\",  date(2020, 4, 16), 7 |]\n",
      "  [| \"orange\", date(2020, 4, 17), 2 |]\n",
      "\n",
      "show scalar \"Date of Orange\" with Orders[Pid == \"orange\"].Date\n",
      "correct:9 out of 15, 60.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "indexes = [f.split('.')[0] for f in os.listdir('mychallenges') if f.endswith('.md') and f!='description.md']\n",
    "pipeline_score_allchallenge(indexes,coder_personality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcher-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
